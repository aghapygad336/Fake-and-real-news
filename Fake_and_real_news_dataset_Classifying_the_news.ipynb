{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake and real news dataset Classifying the news.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrB1S55It6oOkyxK58p0Lq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aghapygad336/Fake-and-real-news/blob/master/Fake_and_real_news_dataset_Classifying_the_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UihMsWIJp4Af",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg1uyntOATS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark==2.4.5\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbUOyoaA2-KC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY60bnIcsdQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG9hkS2vy0DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQMl29ZG2E1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanDF(df):\n",
        "    df.drop(['date', 'title'], axis=1, inplace=True)\n",
        "    df.text = df.text.str.lower()\n",
        "    df.text = df.text.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
        "    df.text = df.text.str.replace(r'[^\\.\\w\\s]','') #remove everything but characters and punctuation\n",
        "    df.text = df.text.str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
        "    df.text = df.text.str.replace(r'\\.',' . ') #replace multple periods with a single one\n",
        "    df.text = df.text.str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
        "    df.text = df.text.str.strip() \n",
        "    print(df.shape)\n",
        "    df.head()\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeRm7Uhl1GOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "True_df = pd.read_csv('/content/drive/My Drive/ask/FakeNews/True.csv')\n",
        "True_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcwwMLk4_RRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "True_df=cleanDF(True_df)\n",
        "True_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy5bQqWb3peT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fake_df = pd.read_csv('/content/drive/My Drive/ask/FakeNews/Fake.csv')\n",
        "Fake_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2BZZlD-9Azb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fake_df=cleanDF(Fake_df)\n",
        "Fake_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "430xnz90ANjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Fake_df['CLASS'] = 0\n",
        "Fake_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPDx8FasP1Eb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "True_df['CLASS'] = 1\n",
        "True_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvKtGZYHSmHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "frames = [Fake_df,True_df]\n",
        "\n",
        "Data_df = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBdNYx--TBde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4rpPWFutWlr",
        "colab_type": "text"
      },
      "source": [
        "Vectorization -- Word2Vec\n",
        "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n",
        "\n",
        "Word embedding is the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcrzHfdhcS_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = Data_df[\"CLASS\"].values\n",
        "#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\n",
        "X = []\n",
        "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "for par in Data_df[\"text\"].values:\n",
        "    tmp = []\n",
        "    sentences = nltk.sent_tokenize(par)\n",
        "    for sent in sentences:\n",
        "        sent = sent.lower()\n",
        "        tokens = tokenizer.tokenize(sent)\n",
        "        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n",
        "        tmp.extend(filtered_words)\n",
        "    X.append(tmp)\n",
        "\n",
        "del Data_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBSFGy6E6Yrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "889UcrTm0RuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "#Dimension of vectors we are generating\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
        "w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDmBTcdy1X2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(w2v_model.wv.vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXWrD4KQ1tHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizing Text -> Repsesenting each word by a number\n",
        "# Mapping of orginal word to number is preserved in word_index property of tokenizer\n",
        "\n",
        "#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwdLO3343cfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones\n",
        "maxlen = 700 \n",
        "#Making all news of size maxlen defined above\n",
        "X = pad_sequences(X, maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6E847V43fQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Edq_9i2430nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets check few word to numerical replesentation\n",
        "#Mapping is preserved in dictionary -> word_index property of instance\n",
        "word_index = tokenizer.word_index\n",
        "for word, num in word_index.items():\n",
        "    print(f\"{word} -> {num}\")\n",
        "    if num == 10:\n",
        "        break  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltzJ5F3g3kV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to create weight matrix from word2vec gensim model\n",
        "def get_weight_matrix(model, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model[word]\n",
        "    return weight_matrix\n",
        "embedding_vectors = get_weight_matrix(w2v_model, word_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtHbNN993986",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVzljZ1H6B3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDVEWelH4j4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "MAX_NB_WORDS = 60000\n",
        "EMBEDDING_DIM = 5000\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtkftM6gaBKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.regularizers import l2\n",
        "from keras import regularizers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x_vdp_84ACP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "#Defining Neural Network\n",
        "#Non-trainable embeddidng layer\n",
        "#LSTM \n",
        "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
        "model.add(LSTM(units=64 , recurrent_dropout = 0.2 , dropout = 0.5))\n",
        "model.add(Dense(units = 32 , activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.layers[0].trainable = False\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_lstm =model.fit(X_train, y_train, epochs=20 ,batch_size=128, workers=10, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmkWbjt5cymN",
        "colab_type": "text"
      },
      "source": [
        "$ANALYSIS AFTER TRAINING OF MODEL$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN-R58W05rcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(X_train,y_train)[1]*100)\n",
        "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQc8Hw6WcwB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(history_lstm.history['loss'])\n",
        "plt.plot(history_lstm.history['val_loss'])\n",
        "plt.title(\"Train vs Validation Loss\")\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.grid(False)\n",
        "plt.legend(['train', 'test'], loc='upper right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V82ThgdhjOLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "pred = model.predict_classes(X_test)\n",
        "cm = confusion_matrix(y_test,pred)\n",
        "cm = pd.DataFrame(cm , index = ['Fake','Real'] , columns = ['Fake','Real'])\n",
        "plt.figure(figsize = (10,10))\n",
        "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Real'] , yticklabels = ['Fake','Real'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}